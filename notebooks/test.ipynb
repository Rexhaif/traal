{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.filesystems import S3FileSystem\n",
    "from datasets import DatasetDict, load_metric, Dataset\n",
    "import torch\n",
    "import transformers as tr\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = tr.AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3fs = S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict.load_from_disk(\"s3://traal-storage/datasets/conll2003\", fs=s3fs)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mnlp_score(probas) -> np.ndarray:\n",
    "    return np.array([-np.sum(np.log(np.max(i, axis=1))) / len(i) for i in probas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_fn(examples):\n",
    "    inputs = tokenizer(examples['tokens'], truncation=True, padding='longest', return_tensors='pt', is_split_into_words=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "        outputs = torch.softmax(outputs, -1).numpy()\n",
    "    \n",
    "    uncertainty_estimates = calculate_mnlp_score(outputs)\n",
    "    return {'score': uncertainty_estimates}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3e963c951f47aea615c44805bc57de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/878 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/rexhaif/traal/notebooks/test.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btech.theping.co/home/rexhaif/traal/notebooks/test.ipynb#ch0000040vscode-remote?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(apply_model_fn, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py:2346\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2342'>2343</a>\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2344'>2345</a>\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2345'>2346</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2346'>2347</a>\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2347'>2348</a>\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2348'>2349</a>\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2349'>2350</a>\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2350'>2351</a>\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2351'>2352</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2352'>2353</a>\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2353'>2354</a>\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2354'>2355</a>\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2355'>2356</a>\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2356'>2357</a>\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2357'>2358</a>\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2358'>2359</a>\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2359'>2360</a>\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2360'>2361</a>\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2361'>2362</a>\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2362'>2363</a>\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2363'>2364</a>\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2364'>2365</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2365'>2366</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2367'>2368</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py:532\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=529'>530</a>\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=530'>531</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=531'>532</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=532'>533</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=533'>534</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=534'>535</a>\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py:499\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=491'>492</a>\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=492'>493</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=493'>494</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=494'>495</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=495'>496</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=496'>497</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=497'>498</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=498'>499</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=499'>500</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=500'>501</a>\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/fingerprint.py?line=451'>452</a>\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/fingerprint.py?line=452'>453</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/fingerprint.py?line=453'>454</a>\u001b[0m             )\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/fingerprint.py?line=455'>456</a>\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/fingerprint.py?line=457'>458</a>\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/fingerprint.py?line=459'>460</a>\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/fingerprint.py?line=461'>462</a>\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py:2734\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2729'>2730</a>\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2730'>2731</a>\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2731'>2732</a>\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2732'>2733</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2733'>2734</a>\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2734'>2735</a>\u001b[0m         batch,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2735'>2736</a>\u001b[0m         indices,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2736'>2737</a>\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2737'>2738</a>\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2738'>2739</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2739'>2740</a>\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2740'>2741</a>\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2741'>2742</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2742'>2743</a>\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py:2614\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2611'>2612</a>\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2612'>2613</a>\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2613'>2614</a>\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2614'>2615</a>\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2615'>2616</a>\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2616'>2617</a>\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py:2306\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2301'>2302</a>\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2302'>2303</a>\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2303'>2304</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2304'>2305</a>\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2305'>2306</a>\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2306'>2307</a>\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/datasets/arrow_dataset.py?line=2307'>2308</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
      "\u001b[1;32m/home/rexhaif/traal/notebooks/test.ipynb Cell 8'\u001b[0m in \u001b[0;36mapply_model_fn\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btech.theping.co/home/rexhaif/traal/notebooks/test.ipynb#ch0000038vscode-remote?line=1'>2</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(examples[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m'\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, is_split_into_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btech.theping.co/home/rexhaif/traal/notebooks/test.ipynb#ch0000038vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btech.theping.co/home/rexhaif/traal/notebooks/test.ipynb#ch0000038vscode-remote?line=3'>4</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btech.theping.co/home/rexhaif/traal/notebooks/test.ipynb#ch0000038vscode-remote?line=4'>5</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(outputs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btech.theping.co/home/rexhaif/traal/notebooks/test.ipynb#ch0000038vscode-remote?line=6'>7</a>\u001b[0m uncertainty_estimates \u001b[39m=\u001b[39m calculate_mnlp_score(outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1640\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1636'>1637</a>\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1638'>1639</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1639'>1640</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1640'>1641</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1641'>1642</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1642'>1643</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1643'>1644</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1644'>1645</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1645'>1646</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1646'>1647</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1647'>1648</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1648'>1649</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1649'>1650</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1650'>1651</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1651'>1652</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1652'>1653</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1654'>1655</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=1656'>1657</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:932\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=929'>930</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=930'>931</a>\u001b[0m     err_msg_prefix \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=931'>932</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either \u001b[39m\u001b[39m{\u001b[39;00merr_msg_prefix\u001b[39m}\u001b[39;00m\u001b[39minput_ids or \u001b[39m\u001b[39m{\u001b[39;00merr_msg_prefix\u001b[39m}\u001b[39;00m\u001b[39minputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=933'>934</a>\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/rexhaif/miniconda3/envs/traal/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py?line=934'>935</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(apply_model_fn, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [['Police',\n",
       "   'said',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'crew',\n",
       "   'members',\n",
       "   'had',\n",
       "   'left',\n",
       "   'the',\n",
       "   'aircraft',\n",
       "   'and',\n",
       "   'said',\n",
       "   'details',\n",
       "   'would',\n",
       "   'be',\n",
       "   'given',\n",
       "   'at',\n",
       "   'a',\n",
       "   'news',\n",
       "   'conference',\n",
       "   'expected',\n",
       "   'to',\n",
       "   'be',\n",
       "   'held',\n",
       "   'in',\n",
       "   'the',\n",
       "   'next',\n",
       "   'few',\n",
       "   'minutes',\n",
       "   'by',\n",
       "   'the',\n",
       "   'local',\n",
       "   'police',\n",
       "   'chief',\n",
       "   '.'],\n",
       "  ['At',\n",
       "   'about',\n",
       "   '4',\n",
       "   'a.m.',\n",
       "   'EDT',\n",
       "   '(',\n",
       "   '0800',\n",
       "   'GMT',\n",
       "   ')',\n",
       "   ',',\n",
       "   'a',\n",
       "   'group',\n",
       "   'of',\n",
       "   'teenaged',\n",
       "   'girls',\n",
       "   'were',\n",
       "   'having',\n",
       "   'the',\n",
       "   'overnight',\n",
       "   'party',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Camelot',\n",
       "   'subdivision',\n",
       "   'of',\n",
       "   'this',\n",
       "   'eastern',\n",
       "   'Virginia',\n",
       "   'city',\n",
       "   ',',\n",
       "   'when',\n",
       "   'a',\n",
       "   'man',\n",
       "   'entered',\n",
       "   'the',\n",
       "   'house',\n",
       "   ',',\n",
       "   'wielding',\n",
       "   'a',\n",
       "   'knife',\n",
       "   ',',\n",
       "   'threatening',\n",
       "   'to',\n",
       "   'sexually',\n",
       "   'assault',\n",
       "   'the',\n",
       "   'girls',\n",
       "   '.']],\n",
       " 'ner': [['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-MISC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']],\n",
       " 'score': [1.5584157618080698, 1.5612000012006917]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[np.argsort(dataset['score'])[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = ['O', 'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER']\n",
    "label2id = {l:i for i, l in enumerate(id2label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return tr.AutoModelForTokenClassification.from_pretrained(\"distilbert-base-cased\", num_labels=len(id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_to_ids(item):\n",
    "    return {\n",
    "        'ner': [label2id[x] for x in item['ner']]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e1619fb40a4596a29d8504e04f06ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/7021 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08cd15967ad4150ae31d72df19c5495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/7021 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638d89c78f4d40509a931fb0065a02de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1626 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf43d6d17ce441781b311a358771683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1625 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7529bae80f9446f2b2409af0997c486b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1727 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aa25eea859402aa4ac70a8414b4f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1727 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(convert_label_to_ids, batched=False, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tr.AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896cb3ab5e6c4cc381568a3f822817ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7d18df3b9e47c9a2697e6f9ff78009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d503e556ff74857990d18f84e748487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbab5dbf256e439b85c49ccd106cd3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adff24cfb0f47c7afc05ef34c45a70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b69ebd120f4e73996095661ec5b649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize_and_align_labels, batched=True, batch_size=128, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_tokens(dataset: Dataset) -> int:\n",
    "    counter = 0\n",
    "    for row in dataset:\n",
    "        counter += len(row['tokens'])\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203621"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_n_tokens(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = tr.DataCollatorForTokenClassification(tokenizer=tokenizer, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "def create_experiment_id() -> str:\n",
    "    base_uuid = str(uuid4()).split(\"-\")[-1]\n",
    "    return base_uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d151ffe85bac'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_experiment_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's3fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rexhaif/traal/notebooks/test.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btech.theping.co/home/rexhaif/traal/notebooks/test.ipynb#ch0000035vscode-remote?line=0'>1</a>\u001b[0m s3fs\n",
      "\u001b[0;31mNameError\u001b[0m: name 's3fs' is not defined"
     ]
    }
   ],
   "source": [
    "s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_time_for_saving():\n",
    "    current_datatime = datetime.now()\n",
    "    return current_datatime.strftime(\"%m-%d-%a-%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05-16-Mon-04-58'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_time_for_saving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /home/rexhaif/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.19.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /home/rexhaif/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "experiment_id = create_experiment_id()\n",
    "experiment_time = get_time_for_saving()\n",
    "experiment_type = \"conll2003\"\n",
    "experiment_seed = 42\n",
    "output_dir = Path(f\"../experiments/{experiment_type}/{experiment_seed}/{experiment_time}-{experiment_id}\")\n",
    "hp_search_dir = output_dir / \"hp_search\"\n",
    "\n",
    "training_args = tr.TrainingArguments(\n",
    "    output_dir=hp_search_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy='epoch',\n",
    "    disable_tqdm=False,\n",
    "    group_by_length=True,\n",
    "    seed=experiment_seed,\n",
    "    fp16=True, fp16_opt_level=\"O2\",\n",
    "    metric_for_best_model='eval_f1',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "trainer = tr.Trainer(\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    model_init=model_init,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def custom_search_objective(trial: optuna.Trial):\n",
    "    possible_batch_sizes = [4, 8, 16, 32, 64]\n",
    "    possible_batch_sizes = [x for x in possible_batch_sizes if (len(dataset['train']) / x) >= 20.0]\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.0, 0.3, step=0.01),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1, step=0.01),\n",
    "        \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", ['linear', 'constant', 'cosine']),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", possible_batch_sizes)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_objective(metrics):\n",
    "    return metrics['eval_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = trainer.hyperparameter_search(\n",
    "    hp_space=custom_search_objective,\n",
    "    compute_objective=compute_objective,\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    n_trials=10 # number of trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_experiment_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rexhaif/traal/notebooks/test.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btech.theping.co/home/rexhaif/traal/notebooks/test.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m create_experiment_id()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_experiment_id' is not defined"
     ]
    }
   ],
   "source": [
    "create_experiment_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_dir(base_dir: str, best_trial: tr.trainer_utils.BestRun):\n",
    "    best_run_dir = Path(base_dir, f\"run-{best_trial.run_id}\")\n",
    "    max_step = -1\n",
    "    max_step_path = None\n",
    "    for dir in best_run_dir.iterdir():\n",
    "        step_n = int(dir.parts[-1].split(\"-\")[1])\n",
    "        if step_n > max_step:\n",
    "            max_step = step_n\n",
    "            max_step_path = dir\n",
    "\n",
    "    return max_step_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = get_checkpoint_dir(hp_search_dir, best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(output_dir) / \"models\" / \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../experiments/conll2003/42/05-15-Sun-18-10-7bfd6675c8d0/models/full')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir.replace(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(hp_search_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t   rng_state.pth  special_tokens_map.json  trainer_state.json\n",
      "optimizer.pt\t   scaler.pt\t  tokenizer_config.json    training_args.bin\n",
      "pytorch_model.bin  scheduler.pt   tokenizer.json\t   vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rexhaif/traal/experiments/conll2003/42/05-15-Sun-18-10-7bfd6675c8d0/models/full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f41d256f1e3e1c12c1f93a292794af53d9a90d2650ec3c9288b49cab993c00bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('traal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
